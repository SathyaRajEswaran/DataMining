{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab973c1f-34b0-4cd6-85d8-35814071fd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Title: Assignment 9.2\n",
    "# Author: Sathya Raj Eswaran\n",
    "# Date: 11 Nov 2025\n",
    "# Modified By: Sathya Raj Eswaran\n",
    "# Description: Best Model Selection and Hyperparameter Tuning\n",
    "# Data: https://www.kaggle.com/datasets/granjithkumar/loan-approval-data-set\n",
    "# ----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6973c491-787f-4ba1-829b-b3f33b4cabaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66df3478-a40a-4977-b962-3f0a4fd89a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded\n",
      "\n",
      "First 5 rows of the DataFrame:\n",
      "    Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
      "0  LP001002   Male      No          0      Graduate            No   \n",
      "1  LP001003   Male     Yes          1      Graduate            No   \n",
      "2  LP001005   Male     Yes          0      Graduate           Yes   \n",
      "3  LP001006   Male     Yes          0  Not Graduate            No   \n",
      "4  LP001008   Male      No          0      Graduate            No   \n",
      "\n",
      "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "0             5849                0.0         NaN             360.0   \n",
      "1             4583             1508.0       128.0             360.0   \n",
      "2             3000                0.0        66.0             360.0   \n",
      "3             2583             2358.0       120.0             360.0   \n",
      "4             6000                0.0       141.0             360.0   \n",
      "\n",
      "   Credit_History Property_Area Loan_Status  \n",
      "0             1.0         Urban           Y  \n",
      "1             1.0         Rural           N  \n",
      "2             1.0         Urban           Y  \n",
      "3             1.0         Urban           Y  \n",
      "4             1.0         Urban           Y  \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"data\\\\Loan_Train.csv\")\n",
    "# Display the first 5 rows to visually inspect the data.\n",
    "print(f\"Successfully loaded\")\n",
    "print(\"\\nFirst 5 rows of the DataFrame:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e8183-e229-45bd-a140-b49440457f3d",
   "metadata": {},
   "source": [
    "### Data Preparation for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddba91bf-2398-4f5f-8d11-6edcfd98abfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before dropping Loan_ID: (614, 13)\n",
      "Shape after dropping Loan_ID: (614, 12)\n"
     ]
    }
   ],
   "source": [
    "# Drop the column \"Loan_ID\"\n",
    "# This is an identifier column and is not useful for model training.\n",
    "print(\"Shape before dropping Loan_ID:\", df.shape)\n",
    "df = df.drop('Loan_ID', axis=1)\n",
    "print(\"Shape after dropping Loan_ID:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7119d83-3369-4275-a0a5-1a2432b9e285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of rows before dropping missing values: 614\n",
      "Number of rows after dropping missing values: 480\n"
     ]
    }
   ],
   "source": [
    "# Drop any rows with missing data \n",
    "print(\"\\nNumber of rows before dropping missing values:\", len(df))\n",
    "df = df.dropna()\n",
    "print(\"Number of rows after dropping missing values:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cde5444-cb50-40f6-b07f-f8db80da5d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape before creating dummy variables: (480, 12)\n",
      "Shape after creating dummy variables: (480, 15)\n"
     ]
    }
   ],
   "source": [
    "# Convert the categorical features into dummy variables (One-Hot Encoding)\n",
    "# pandas get_dummies automatically identifies and converts object/category types\n",
    "print(\"\\nShape before creating dummy variables:\", df.shape)\n",
    "df_model = pd.get_dummies(df, drop_first=True)\n",
    "print(\"Shape after creating dummy variables:\", df_model.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "381e0ec8-2554-46eb-b931-941f0b5125d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Final Prepared DataFrame (df_model) Head \n",
      "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "1             4583             1508.0       128.0             360.0   \n",
      "2             3000                0.0        66.0             360.0   \n",
      "3             2583             2358.0       120.0             360.0   \n",
      "4             6000                0.0       141.0             360.0   \n",
      "5             5417             4196.0       267.0             360.0   \n",
      "\n",
      "   Credit_History  Gender_Male  Married_Yes  Dependents_1  Dependents_2  \\\n",
      "1             1.0         True         True          True         False   \n",
      "2             1.0         True         True         False         False   \n",
      "3             1.0         True         True         False         False   \n",
      "4             1.0         True        False         False         False   \n",
      "5             1.0         True         True         False          True   \n",
      "\n",
      "   Dependents_3+  Education_Not Graduate  Self_Employed_Yes  \\\n",
      "1          False                   False              False   \n",
      "2          False                   False               True   \n",
      "3          False                    True              False   \n",
      "4          False                   False              False   \n",
      "5          False                   False               True   \n",
      "\n",
      "   Property_Area_Semiurban  Property_Area_Urban  Loan_Status_Y  \n",
      "1                    False                False          False  \n",
      "2                    False                 True           True  \n",
      "3                    False                 True           True  \n",
      "4                    False                 True           True  \n",
      "5                    False                 True           True  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display the first few rows of the final prepared DataFrame\n",
    "print(\"\\n Final Prepared DataFrame (df_model) Head \")\n",
    "print(df_model.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff47f90-0c8c-4721-b9b9-b6235f431905",
   "metadata": {},
   "source": [
    "### Split the data into a training and test set, where the “Loan_Status” column is the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2850df76-b8ef-469c-8ac6-9f51984d7463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data Split Verification \n",
      "Total features (X) shape: (480, 14)\n",
      "Total target (y) shape: (480,)\n",
      "-----------------------------------\n",
      "X_train shape: (384, 14)\n",
      "X_test shape: (96, 14)\n",
      "y_train shape: (384,)\n",
      "y_test shape: (96,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define Features (X) and Target (y)\n",
    "target_column = 'Loan_Status_Y' \n",
    "\n",
    "# X contains all columns except the target\n",
    "X = df_model.drop(target_column, axis=1)\n",
    "\n",
    "# y contains only the target column\n",
    "y = df_model[target_column]\n",
    "\n",
    "# Split the data into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y # Stratify ensures the train and test sets have the same proportion of target classes\n",
    ")\n",
    "\n",
    "# Print the resulting shapes to verify the split\n",
    "print(\" Data Split Verification \")\n",
    "print(f\"Total features (X) shape: {X.shape}\")\n",
    "print(f\"Total target (y) shape: {y.shape}\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85e7738-eb9c-497a-842e-c43db4f0b820",
   "metadata": {},
   "source": [
    "### Create a pipeline with a min-max scaler and a KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec98ede1-4ce1-4e0d-9b56-87d548787309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Pipeline Structure ---\n",
      "Pipeline(steps=[('scaler', MinMaxScaler()), ('knn', KNeighborsClassifier())])\n",
      "----------------------------\n",
      "Pipeline created successfully. It is ready for training (fitting).\n"
     ]
    }
   ],
   "source": [
    "# Define the steps in the pipeline:\n",
    "# 1. 'scaler': MinMaxScaler\n",
    "# 2. 'knn': KNeighborsClassifier (using default k=5)\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Print the pipeline structure to confirm the steps\n",
    "print(\"--- Pipeline Structure ---\")\n",
    "print(pipeline)\n",
    "print(\"-\" * 28)\n",
    "print(\"Pipeline created successfully. It is ready for training (fitting).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8796fc2-a3aa-4715-9090-d3da87f0934f",
   "metadata": {},
   "source": [
    "### Create a search space for your KNN classifier where the “n_neighbors” parameter varies from 1 to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1681d243-2b58-48e4-a3e3-bb0f5c8987be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the KNN Pipeline to the training data (X_train, y_train)...\n",
      "Pipeline fitting complete.\n"
     ]
    }
   ],
   "source": [
    "# Fit the pipeline to the training data\n",
    "print(\"Fitting the KNN Pipeline to the training data (X_train, y_train)...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Pipeline fitting complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed436575-5b2b-4841-aff9-fbb07c020393",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the pipeline on the test data\n",
    "# The score method automatically calculates the accuracy for classification models.\n",
    "accuracy = pipeline.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a63a1fc-67d2-40c1-85e4-ab3bd1d2bf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Performance on Test Set ---\n",
      "Model Accuracy on the Test Set: 0.7083\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Report the accuracy\n",
    "print(\"\\n--- Model Performance on Test Set ---\")\n",
    "print(f\"Model Accuracy on the Test Set: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8272ee-eddb-4b1c-96b1-0f418b31e56f",
   "metadata": {},
   "source": [
    "### Fit a default KNN classifier to the data with this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b96efad-faf5-44ae-bb29-f34a1f77a0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the KNN Pipeline to the training data (X_train, y_train)...\n",
      "Pipeline fitting complete.\n",
      "\n",
      "--- Model Performance on Test Set ---\n",
      "Model Accuracy on the Test Set: 0.7083\n"
     ]
    }
   ],
   "source": [
    "# Fit the pipeline to the training data\n",
    "print(\"Fitting the KNN Pipeline to the training data (X_train, y_train)...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Pipeline fitting complete.\")\n",
    "\n",
    "# Evaluate the pipeline on the test data\n",
    "# The score method automatically calculates the accuracy for classification models.\n",
    "accuracy = pipeline.score(X_test, y_test)\n",
    "\n",
    "# Report the accuracy\n",
    "print(\"\\n--- Model Performance on Test Set ---\")\n",
    "print(f\"Model Accuracy on the Test Set: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48237d4-79a5-4797-a041-a5ca1de85e28",
   "metadata": {},
   "source": [
    "### Find the accuracy of the grid search best model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "410eb62d-80b0-40c4-8de8-cde1e78b227e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Grid Search (testing 40 different models)...\n",
      "Grid Search complete.\n",
      "\n",
      "--- Grid Search Results ---\n",
      "Best cross-validation accuracy: 0.7604\n",
      "Best parameters found: {'knn__n_neighbors': 20, 'knn__weights': 'distance'}\n",
      "\n",
      "--- Best Model Performance on Test Set ---\n",
      "Accuracy of the Grid Search Best Model on the Test Set: 0.7188\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'knn__n_neighbors': range(1, 21),  # Test k from 1 to 20\n",
    "    'knn__weights': ['uniform', 'distance'] # Test uniform and distance weighting\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "# cv=5 means 5-fold cross-validation will be used on the training data.\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "# This step performs the cross-validation and hyperparameter tuning.\n",
    "print(\"Starting Grid Search (testing 40 different models)...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Grid Search complete.\")\n",
    "\n",
    "# Report the Best Parameters found\n",
    "print(\"\\n--- Grid Search Results ---\")\n",
    "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate the best estimator on the test set\n",
    "# The 'best_estimator_' attribute is the fitted pipeline with the optimal parameters.\n",
    "best_model = grid_search.best_estimator_\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "\n",
    "# Report the accuracy of the best model on the test set\n",
    "print(\"\\n--- Best Model Performance on Test Set ---\")\n",
    "print(f\"Accuracy of the Grid Search Best Model on the Test Set: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6907ab81-3b70-4e92-b34f-6c7b1cbb0c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# --- Define the Multi-Model Pipeline ---\n",
    "# The classifier is just a placeholder; the grid search will swap it out.\n",
    "pipeline_multi = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('classifier', LogisticRegression()) \n",
    "])\n",
    "\n",
    "# --- Define the Expanded Parameter Grids ---\n",
    "\n",
    "# Grid 1: K-Nearest Neighbors (KNN)\n",
    "knn_params = {\n",
    "    'classifier': [KNeighborsClassifier()],\n",
    "    'classifier__n_neighbors': range(1, 21),\n",
    "    'classifier__weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "# Grid 2: Logistic Regression (LogReg)\n",
    "# Based on Section 12.3: L1/L2 and C logspace\n",
    "logreg_params = {\n",
    "    'classifier': [LogisticRegression(random_state=42, solver='liblinear')],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__C': np.logspace(-4, 4, 10) # 10 values from 0.0001 to 10000\n",
    "}\n",
    "\n",
    "# Grid 3: Random Forest (RF)\n",
    "# Based on Section 12.3: n_estimators, max_features, max_depth\n",
    "forest_params = {\n",
    "    'classifier': [RandomForestClassifier(random_state=42)],\n",
    "    'classifier__n_estimators': [10, 50, 100], \n",
    "    'classifier__max_features': [1, 2, 'sqrt'], \n",
    "    'classifier__max_depth': [None, 5, 10]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "505e87f0-672b-4a1a-ba28-cfc95ed62d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting combined Grid Search across KNN, Logistic Regression, and Random Forest...\n",
      "Grid Search complete.\n",
      "\n",
      "--- Combined Grid Search Results ---\n",
      "Best Model Found: LogisticRegression\n",
      "Best Cross-Validation Accuracy: 0.8020\n",
      "Best Parameters Found: {'classifier': LogisticRegression(random_state=42, solver='liblinear'), 'classifier__C': 0.046415888336127774, 'classifier__penalty': 'l1'}\n",
      "\n",
      "--- Best Model Performance on Test Set ---\n",
      "Accuracy of the Grid Search Best Model on the Test Set: 0.8333\n"
     ]
    }
   ],
   "source": [
    "# Combine all parameter grids into a list\n",
    "param_grids = [knn_params, logreg_params, forest_params]\n",
    "\n",
    "# --- Initialize and Run Grid Search ---\n",
    "# We pass the list of grids to param_grid\n",
    "grid_search_multi = GridSearchCV(\n",
    "    estimator=pipeline_multi,\n",
    "    param_grid=param_grids,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1 # Use all available cores\n",
    ")\n",
    "\n",
    "print(\"Starting combined Grid Search across KNN, Logistic Regression, and Random Forest...\")\n",
    "# Fit the grid search (assuming X_train, y_train are in memory)\n",
    "grid_search_multi.fit(X_train, y_train)\n",
    "print(\"Grid Search complete.\")\n",
    "\n",
    "# --- Report Results ---\n",
    "\n",
    "# Find the best model's class name for reporting\n",
    "best_model_name = type(grid_search_multi.best_estimator_.named_steps['classifier']).__name__\n",
    "\n",
    "print(\"\\n--- Combined Grid Search Results ---\")\n",
    "print(f\"Best Model Found: {best_model_name}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search_multi.best_score_:.4f}\")\n",
    "print(f\"Best Parameters Found: {grid_search_multi.best_params_}\")\n",
    "\n",
    "# --- Report Accuracy on the Test Set ---\n",
    "# Use the best_estimator_ found by the search to score the test set\n",
    "test_accuracy = grid_search_multi.score(X_test, y_test)\n",
    "\n",
    "print(\"\\n--- Best Model Performance on Test Set ---\")\n",
    "print(f\"Accuracy of the Grid Search Best Model on the Test Set: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d778d9e-5c9d-4d92-b684-da0253c4d557",
   "metadata": {},
   "source": [
    "### Key Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6e411-affe-4ebe-82ca-fc4b14a078ab",
   "metadata": {},
   "source": [
    "The analysis confirms that model selection and targeted hyperparameter tuning are the primary drivers of predictive performance.Model Performance and Algorithm SuitabilityInitial KNN Assessment: Data scaling, while necessary, yielded only modest predictive performance with the default K-Nearest Neighbors (KNN) model (70.83% accuracy). Subsequent hyperparameter tuning of the KNN model resulted in only a marginal improvement (+1.05%), suggesting that the KNN algorithm is fundamentally not the optimal fit for this specific dataset.Superiority of Logistic Regression: The comprehensive multi-model grid search unequivocally identified the Logistic Regression model as the superior choice. It achieved the highest test accuracy of 83.33%, representing a significant gain of over 11 percentage points compared to the tuned KNN model. This reinforces the principle that selecting the correct base algorithm often outweighs extensive tuning of a sub-optimal one.Optimal Hyperparameter ConfigurationRole of Regularization: The peak performance was attained using Logistic Regression with the optimal hyperparameters identified as penalty='l1' and a specific regularization strength value of $C \\approx 0.046$.This outcome highlights that L1 regularization is crucial for achieving maximum performance on this loan approval dataset, as it provides a mechanism for robust feature selection and complexity control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd8ce65-c883-4a26-8de8-3d65bbb38ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
